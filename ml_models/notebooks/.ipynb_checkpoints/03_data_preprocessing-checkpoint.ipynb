{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Data Preprocessing & Feature Engineering\n",
    "\n",
    "This notebook covers:\n",
    "1. Data cleaning and preprocessing\n",
    "2. Feature encoding and scaling\n",
    "3. Feature engineering and selection\n",
    "4. Data splitting for ML\n",
    "5. Preparation for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Scikit-learn preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import joblib\n",
    "\n",
    "# Set up paths\n",
    "DATA_PROCESSED = Path('../data/processed')\n",
    "MODELS = Path('../models')\n",
    "MODELS.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported and paths set up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ingredient database\n",
    "print(\"üìä Loading ingredient database...\")\n",
    "ingredient_db = pd.read_csv(DATA_PROCESSED / 'ingredient_toxicity_db.csv')\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(ingredient_db)} ingredients\")\n",
    "print(f\"Columns: {list(ingredient_db.columns)}\")\n",
    "print(f\"Shape: {ingredient_db.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "ingredient_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing pipeline\n",
    "print(\"üîß PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = ingredient_db.copy()\n",
    "\n",
    "# 1. Handle missing values (if any)\n",
    "print(\"\\n1Ô∏è‚É£ Checking for missing values...\")\n",
    "missing_values = df_processed.isnull().sum()\n",
    "print(f\"Missing values: {missing_values.sum()}\")\n",
    "if missing_values.sum() > 0:\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found\")\n",
    "\n",
    "# 2. Remove duplicates (if any)\n",
    "print(\"\\n2Ô∏è‚É£ Checking for duplicates...\")\n",
    "duplicates = df_processed.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    print(f\"‚úÖ Removed {duplicates} duplicate rows\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found\")\n",
    "\n",
    "# 3. Data type optimization\n",
    "print(\"\\n3Ô∏è‚É£ Optimizing data types...\")\n",
    "print(\"Before optimization:\")\n",
    "print(df_processed.dtypes)\n",
    "\n",
    "# Convert categorical columns to category type\n",
    "categorical_cols = ['category', 'health_impact', 'risk_level', 'allergen_risk']\n",
    "for col in categorical_cols:\n",
    "    if col in df_processed.columns:\n",
    "        df_processed[col] = df_processed[col].astype('category')\n",
    "\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(df_processed.dtypes)\n",
    "print(\"‚úÖ Data types optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature encoding\n",
    "print(\"üè∑Ô∏è FEATURE ENCODING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Separate features and target\n",
    "target_col = 'is_toxic'\n",
    "feature_cols = [col for col in df_processed.columns if col not in [target_col, 'ingredient_name']]\n",
    "\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "# Initialize encoders\n",
    "label_encoders = {}\n",
    "encoded_features = df_processed[feature_cols].copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_features = encoded_features.select_dtypes(include=['category', 'object']).columns\n",
    "print(f\"\\nCategorical features to encode: {list(categorical_features)}\")\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    encoded_features[f'{col}_encoded'] = le.fit_transform(encoded_features[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"‚úÖ Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Remove original categorical columns\n",
    "encoded_features = encoded_features.drop(columns=categorical_features)\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {encoded_features.shape}\")\n",
    "print(f\"Features: {list(encoded_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "print(\"‚öôÔ∏è FEATURE ENGINEERING\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create additional features\n",
    "engineered_features = encoded_features.copy()\n",
    "\n",
    "# 1. Toxicity level categories\n",
    "engineered_features['toxicity_level'] = pd.cut(\n",
    "    engineered_features['toxicity_score'], \n",
    "    bins=[0, 20, 40, 70, 100], \n",
    "    labels=[0, 1, 2, 3]  # Safe, Low, Medium, High\n",
    ").astype(int)\n",
    "\n",
    "# 2. High toxicity flag\n",
    "engineered_features['high_toxicity_flag'] = (engineered_features['toxicity_score'] > 70).astype(int)\n",
    "\n",
    "# 3. Toxicity score squared (for non-linear relationships)\n",
    "engineered_features['toxicity_score_squared'] = engineered_features['toxicity_score'] ** 2\n",
    "\n",
    "# 4. Toxicity score normalized (0-1 scale)\n",
    "engineered_features['toxicity_score_normalized'] = (\n",
    "    engineered_features['toxicity_score'] / 100\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created {len(engineered_features.columns) - len(encoded_features.columns)} new features\")\n",
    "print(f\"New features: {[col for col in engineered_features.columns if col not in encoded_features.columns]}\")\n",
    "print(f\"Total features: {engineered_features.shape[1]}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\nüìä Feature Statistics:\")\n",
    "engineered_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"üìè FEATURE SCALING\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Prepare features and target\n",
    "X = engineered_features\n",
    "y = df_processed[target_col]\n",
    "\n",
    "print(f\"Feature matrix X: {X.shape}\")\n",
    "print(f\"Target vector y: {y.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Split data before scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Training target distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test target distribution: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\n‚úÖ Features scaled using StandardScaler\")\n",
    "print(f\"Scaled training features mean: {X_train_scaled.mean().mean():.3f}\")\n",
    "print(f\"Scaled training features std: {X_train_scaled.std().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "print(\"üéØ FEATURE SELECTION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 1. Correlation-based feature selection\n",
    "print(\"\\n1Ô∏è‚É£ Correlation Analysis:\")\n",
    "correlation_matrix = X_train_scaled.corr()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"High correlation pairs (>0.8):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} - {feat2}: {corr:.3f}\")\nelse:\n",
    "    print(\"‚úÖ No highly correlated features found\")\n",
    "\n",
    "# 2. Statistical feature selection\n",
    "print(\"\\n2Ô∏è‚É£ Statistical Feature Selection:\")\n",
    "selector = SelectKBest(score_func=f_classif, k='all')\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    'feature': X_train_scaled.columns,\n",
    "    'score': selector.scores_,\n",
    "    'p_value': selector.pvalues_\n",
    "}).sort_values('score', ascending=False)\n",
    "\n",
    "print(\"Feature importance scores:\")\n",
    "print(feature_scores)\n",
    "\n",
    "# Select top features\n",
    "top_k = min(8, len(X_train_scaled.columns))  # Select top 8 features or all if less\n",
    "top_features = feature_scores.head(top_k)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nüéØ Selected top {len(top_features)} features:\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    score = feature_scores[feature_scores['feature'] == feat]['score'].iloc[0]\n",
    "    print(f\"  {i}. {feat}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction (PCA)\n",
    "print(\"üìâ DIMENSIONALITY REDUCTION (PCA)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Original features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"PCA components: {X_train_pca.shape[1]}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "plt.title('PCA Explained Variance by Component')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/pca_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ PCA analysis completed and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data and models\n",
    "print(\"üíæ SAVING PREPROCESSED DATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create final datasets\n",
    "datasets = {\n",
    "    'original': {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    },\n",
    "    'scaled': {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    },\n",
    "    'selected_features': {\n",
    "        'X_train': X_train_scaled[top_features],\n",
    "        'X_test': X_test_scaled[top_features],\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    },\n",
    "    'pca': {\n",
    "        'X_train': pd.DataFrame(X_train_pca, index=X_train.index),\n",
    "        'X_test': pd.DataFrame(X_test_pca, index=X_test.index),\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save datasets\n",
    "for dataset_name, data in datasets.items():\n",
    "    dataset_dir = DATA_PROCESSED / dataset_name\n",
    "    dataset_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for data_name, df in data.items():\n",
    "        filepath = dataset_dir / f'{data_name}.csv'\n",
    "        df.to_csv(filepath, index=True)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {dataset_name} dataset: {data['X_train'].shape}\")\n",
    "\n",
    "# Save preprocessing objects\n",
    "preprocessing_objects = {\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': label_encoders,\n",
    "    'pca': pca,\n",
    "    'feature_selector': selector,\n",
    "    'top_features': top_features\n",
    "}\n",
    "\n",
    "for obj_name, obj in preprocessing_objects.items():\n",
    "    filepath = MODELS / f'{obj_name}.pkl'\n",
    "    joblib.dump(obj, filepath)\n",
    "    print(f\"‚úÖ Saved {obj_name}\")\n",
    "\n",
    "print(f\"\\nüìÅ All preprocessed data saved to: {DATA_PROCESSED}\")\n",
    "print(f\"üìÅ All preprocessing objects saved to: {MODELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing summary\n",
    "print(\"üìã PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "summary = {\n",
    "    'Original Data': {\n",
    "        'Samples': len(ingredient_db),\n",
    "        'Features': len(ingredient_db.columns) - 2,  # Exclude target and ingredient name\n",
    "        'Missing Values': ingredient_db.isnull().sum().sum(),\n",
    "        'Duplicates': ingredient_db.duplicated().sum()\n",
    "    },\n",
    "    'Feature Engineering': {\n",
    "        'Original Features': len(encoded_features.columns),\n",
    "        'Engineered Features': len(engineered_features.columns),\n",
    "        'New Features Created': len(engineered_features.columns) - len(encoded_features.columns)\n",
    "    },\n",
    "    'Data Splitting': {\n",
    "        'Training Samples': len(X_train),\n",
    "        'Test Samples': len(X_test),\n",
    "        'Training Positive Class': y_train.sum(),\n",
    "        'Test Positive Class': y_test.sum()\n",
    "    },\n",
    "    'Feature Selection': {\n",
    "        'Total Features': len(X_train_scaled.columns),\n",
    "        'Selected Features': len(top_features),\n",
    "        'PCA Components': X_train_pca.shape[1],\n",
    "        'PCA Variance Explained': f\"{pca.explained_variance_ratio_.sum():.1%}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for section, metrics in summary.items():\n",
    "    print(f\"\\nüìä {section}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "with open(DATA_PROCESSED / 'preprocessing_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nüíæ Preprocessing summary saved\")\n",
    "print(\"\\nüéâ Week 4 Data Preprocessing Complete!\")\n",
    "print(\"\\nüìã Next Steps (Week 5):\")\n",
    "print(\"  1. Baseline model development\")\n",
    "print(\"  2. Decision Tree implementation\")\n",
    "print(\"  3. Random Forest training\")\n",
    "print(\"  4. Model evaluation and comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}