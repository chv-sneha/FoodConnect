{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Advanced Models & Ensemble Methods\n",
    "\n",
    "This notebook covers:\n",
    "1. Ensemble model creation (Voting, Stacking)\n",
    "2. Neural network implementation\n",
    "3. Model optimization and fine-tuning\n",
    "4. Final model selection and validation\n",
    "5. Production model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"⚠️ TensorFlow not available, using sklearn MLPClassifier instead\")\n",
    "\n",
    "# Set up paths\n",
    "DATA_PROCESSED = Path('../data/processed')\n",
    "MODELS = Path('../models')\n",
    "REPORTS = Path('../reports/figures')\n",
    "\n",
    "print(\"✅ Libraries imported and paths set up\")\n",
    "print(f\"TensorFlow available: {TENSORFLOW_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data and trained models\n",
    "print(\"📊 Loading data and trained models...\")\n",
    "\n",
    "# Load scaled dataset\n",
    "X_train = pd.read_csv(DATA_PROCESSED / 'scaled' / 'X_train.csv', index_col=0)\n",
    "X_test = pd.read_csv(DATA_PROCESSED / 'scaled' / 'X_test.csv', index_col=0)\n",
    "y_train = pd.read_csv(DATA_PROCESSED / 'scaled' / 'y_train.csv', index_col=0).squeeze()\n",
    "y_test = pd.read_csv(DATA_PROCESSED / 'scaled' / 'y_test.csv', index_col=0).squeeze()\n",
    "\n",
    "print(f\"✅ Data loaded: {X_train.shape}, {X_test.shape}\")\n",
    "\n",
    "# Load trained models\n",
    "trained_models = {\n",
    "    'logistic': joblib.load(MODELS / 'baseline_logistic_regression.pkl'),\n",
    "    'decision_tree': joblib.load(MODELS / 'decision_tree_classifier.pkl'),\n",
    "    'random_forest': joblib.load(MODELS / 'random_forest_classifier.pkl'),\n",
    "    'xgboost': joblib.load(MODELS / 'xgboost_classifier.pkl')\n",
    "}\n",
    "\n",
    "print(f\"✅ Loaded {len(trained_models)} trained models\")\n",
    "\n",
    "# Display model performance from Week 5\n",
    "week5_results = pd.read_csv(REPORTS.parent / 'model_comparison_results.csv', index_col=0)\n",
    "print(\"\\n📊 Week 5 Results Summary:\")\n",
    "print(week5_results.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model 1: Voting Classifier\n",
    "print(\"🗳️ ENSEMBLE MODEL 1: VOTING CLASSIFIER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create voting classifier with best performing models\n",
    "voting_estimators = [\n",
    "    ('decision_tree', trained_models['decision_tree']),\n",
    "    ('random_forest', trained_models['random_forest']),\n",
    "    ('xgboost', trained_models['xgboost'])\n",
    "]\n",
    "\n",
    "# Hard voting\n",
    "hard_voting = VotingClassifier(\n",
    "    estimators=voting_estimators,\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "hard_voting.fit(X_train, y_train)\n",
    "hard_pred = hard_voting.predict(X_test)\n",
    "\n",
    "# Soft voting\n",
    "soft_voting = VotingClassifier(\n",
    "    estimators=voting_estimators,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "soft_voting.fit(X_train, y_train)\n",
    "soft_pred = soft_voting.predict(X_test)\n",
    "soft_prob = soft_voting.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate voting classifiers\n",
    "hard_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, hard_pred),\n",
    "    'precision': precision_score(y_test, hard_pred),\n",
    "    'recall': recall_score(y_test, hard_pred),\n",
    "    'f1_score': f1_score(y_test, hard_pred)\n",
    "}\n",
    "\n",
    "soft_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, soft_pred),\n",
    "    'precision': precision_score(y_test, soft_pred),\n",
    "    'recall': recall_score(y_test, soft_pred),\n",
    "    'f1_score': f1_score(y_test, soft_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, soft_prob)\n",
    "}\n",
    "\n",
    "print(\"📊 Hard Voting Results:\")\n",
    "for metric, value in hard_metrics.items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "print(\"\\n📊 Soft Voting Results:\")\n",
    "for metric, value in soft_metrics.items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(soft_voting, X_train, y_train, cv=5, scoring='f1')\n",
    "print(f\"\\n🔄 Soft Voting CV F1-Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Save models\n",
    "joblib.dump(hard_voting, MODELS / 'hard_voting_classifier.pkl')\n",
    "joblib.dump(soft_voting, MODELS / 'soft_voting_classifier.pkl')\n",
    "print(\"\\n✅ Voting classifiers saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model 2: Stacking Classifier\n",
    "print(\"📚 ENSEMBLE MODEL 2: STACKING CLASSIFIER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking_estimators = [\n",
    "    ('decision_tree', trained_models['decision_tree']),\n",
    "    ('random_forest', trained_models['random_forest']),\n",
    "    ('xgboost', trained_models['xgboost'])\n",
    "]\n",
    "\n",
    "stacking_classifier = StackingClassifier(\n",
    "    estimators=stacking_estimators,\n",
    "    final_estimator=trained_models['logistic'],  # Use logistic regression as meta-learner\n",
    "    cv=5,\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "\n",
    "stacking_classifier.fit(X_train, y_train)\n",
    "stacking_pred = stacking_classifier.predict(X_test)\n",
    "stacking_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate stacking classifier\n",
    "stacking_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, stacking_pred),\n",
    "    'precision': precision_score(y_test, stacking_pred),\n",
    "    'recall': recall_score(y_test, stacking_pred),\n",
    "    'f1_score': f1_score(y_test, stacking_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, stacking_prob)\n",
    "}\n",
    "\n",
    "print(\"📊 Stacking Classifier Results:\")\n",
    "for metric, value in stacking_metrics.items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(stacking_classifier, X_train, y_train, cv=5, scoring='f1')\n",
    "print(f\"\\n🔄 Stacking CV F1-Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(stacking_classifier, MODELS / 'stacking_classifier.pkl')\n",
    "print(\"\\n✅ Stacking classifier saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Implementation\n",
    "print(\"🧠 NEURAL NETWORK IMPLEMENTATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"🔥 Using TensorFlow/Keras Neural Network\")\n",
    "    \n",
    "    # Build neural network\n",
    "    def create_neural_network(input_dim):\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(32, activation='relu'),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(16, activation='relu'),\n",
    "            keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Train neural network\n",
    "    nn_model = create_neural_network(X_train.shape[1])\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = nn_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=8,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    nn_prob = nn_model.predict(X_test, verbose=0).flatten()\n",
    "    nn_pred = (nn_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Save model\n",
    "    nn_model.save(MODELS / 'neural_network_model.h5')\n",
    "    \nelse:\n",
    "    print(\"🔧 Using Scikit-learn MLPClassifier\")\n",
    "    \n",
    "    # MLPClassifier as alternative\n",
    "    nn_model = MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32, 16),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    nn_model.fit(X_train, y_train)\n",
    "    nn_pred = nn_model.predict(X_test)\n",
    "    nn_prob = nn_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(nn_model, MODELS / 'neural_network_mlp.pkl')\n",
    "\n",
    "# Evaluate neural network\n",
    "nn_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, nn_pred),\n",
    "    'precision': precision_score(y_test, nn_pred),\n",
    "    'recall': recall_score(y_test, nn_pred),\n",
    "    'f1_score': f1_score(y_test, nn_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, nn_prob)\n",
    "}\n",
    "\n",
    "print(\"\\n📊 Neural Network Results:\")\n",
    "for metric, value in nn_metrics.items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "print(\"\\n✅ Neural network trained and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model comparison\n",
    "print(\"📊 COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compile all results including new models\n",
    "all_advanced_metrics = {\n",
    "    'Hard Voting': hard_metrics,\n",
    "    'Soft Voting': soft_metrics,\n",
    "    'Stacking': stacking_metrics,\n",
    "    'Neural Network': nn_metrics\n",
    "}\n",
    "\n",
    "# Combine with Week 5 results\n",
    "week5_dict = week5_results.to_dict('index')\n",
    "all_metrics_combined = {**week5_dict, **all_advanced_metrics}\n",
    "\n",
    "# Create comprehensive comparison\n",
    "final_comparison = pd.DataFrame(all_metrics_combined).T\n",
    "final_comparison = final_comparison.round(3)\n",
    "\n",
    "print(\"\\n📋 Final Model Comparison:\")\n",
    "print(final_comparison)\n",
    "\n",
    "# Find best models\n",
    "print(\"\\n🏆 Best Models by Metric:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:\n",
    "    if metric in final_comparison.columns:\n",
    "        best_model = final_comparison[metric].idxmax()\n",
    "        best_score = final_comparison[metric].max()\n",
    "        print(f\"  {metric.capitalize()}: {best_model} ({best_score:.3f})\")\n",
    "\n",
    "# Overall best model\n",
    "best_f1_model = final_comparison['f1_score'].idxmax()\n",
    "best_f1_score = final_comparison['f1_score'].max()\n",
    "print(f\"\\n🎯 Overall Best Model (F1-Score): {best_f1_model} ({best_f1_score:.3f})\")\n",
    "\n",
    "# Save final comparison\n",
    "final_comparison.to_csv(REPORTS.parent / 'final_model_comparison.csv')\n",
    "print(\"\\n💾 Final comparison results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model calibration and optimization\n",
    "print(\"⚙️ MODEL CALIBRATION & OPTIMIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get the best model for calibration\n",
    "best_model_name = final_comparison['f1_score'].idxmax()\n",
    "print(f\"\\n🎯 Calibrating best model: {best_model_name}\")\n",
    "\n",
    "# Select the best model object\n",
    "if best_model_name == 'Soft Voting':\n",
    "    best_model = soft_voting\n",
    "elif best_model_name == 'Stacking':\n",
    "    best_model = stacking_classifier\n",
    "elif best_model_name == 'Neural Network':\n",
    "    best_model = nn_model\n",
    "else:\n",
    "    # Use one of the Week 5 models\n",
    "    model_mapping = {\n",
    "        'Decision Tree': 'decision_tree',\n",
    "        'Random Forest': 'random_forest',\n",
    "        'XGBoost': 'xgboost',\n",
    "        'Logistic Regression': 'logistic'\n",
    "    }\n",
    "    best_model = trained_models[model_mapping[best_model_name]]\n",
    "\n",
    "# Calibrate the best model (if it's not neural network)\n",
    "if best_model_name != 'Neural Network' or not TENSORFLOW_AVAILABLE:\n",
    "    calibrated_model = CalibratedClassifierCV(best_model, method='sigmoid', cv=3)\n",
    "    calibrated_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate calibrated model\n",
    "    cal_pred = calibrated_model.predict(X_test)\n",
    "    cal_prob = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    calibrated_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, cal_pred),\n",
    "        'precision': precision_score(y_test, cal_pred),\n",
    "        'recall': recall_score(y_test, cal_pred),\n",
    "        'f1_score': f1_score(y_test, cal_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, cal_prob)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📊 Calibrated Model Results:\")\n",
    "    for metric, value in calibrated_metrics.items():\n",
    "        print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "    \n",
    "    # Save calibrated model\n",
    "    joblib.dump(calibrated_model, MODELS / 'calibrated_best_model.pkl')\n",
    "    print(\"\\n✅ Calibrated model saved\")\n",
    "    \n",
    "    # Compare with original\n",
    "    original_f1 = final_comparison.loc[best_model_name, 'f1_score']\n",
    "    calibrated_f1 = calibrated_metrics['f1_score']\n",
    "    improvement = calibrated_f1 - original_f1\n",
    "    print(f\"\\n📈 F1-Score improvement: {improvement:+.3f}\")\nelse:\n",
    "    print(\"\\n⚠️ Neural network already optimized, skipping calibration\")\n",
    "    calibrated_metrics = nn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model selection and production preparation\n",
    "print(\"🎯 FINAL MODEL SELECTION & PRODUCTION PREP\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Determine production model\n",
    "if 'calibrated_metrics' in locals() and calibrated_metrics['f1_score'] > final_comparison['f1_score'].max():\n",
    "    production_model_name = f\"Calibrated {best_model_name}\"\n",
    "    production_metrics = calibrated_metrics\n",
    "    production_model = calibrated_model\nelse:\n",
    "    production_model_name = best_model_name\n",
    "    production_metrics = final_comparison.loc[best_model_name].to_dict()\n",
    "    production_model = best_model\n",
    "\n",
    "print(f\"\\n🏆 PRODUCTION MODEL: {production_model_name}\")\n",
    "print(\"\\n📊 Production Model Performance:\")\n",
    "for metric, value in production_metrics.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "# Model validation with stratified k-fold\n",
    "print(\"\\n🔄 Final Model Validation (Stratified 5-Fold CV):\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "if production_model_name != 'Neural Network' or not TENSORFLOW_AVAILABLE:\n",
    "    cv_scores = {\n",
    "        'accuracy': cross_val_score(production_model, X_train, y_train, cv=skf, scoring='accuracy'),\n",
    "        'precision': cross_val_score(production_model, X_train, y_train, cv=skf, scoring='precision'),\n",
    "        'recall': cross_val_score(production_model, X_train, y_train, cv=skf, scoring='recall'),\n",
    "        'f1_score': cross_val_score(production_model, X_train, y_train, cv=skf, scoring='f1')\n",
    "    }\n",
    "    \n",
    "    for metric, scores in cv_scores.items():\n",
    "        mean_score = scores.mean()\n",
    "        std_score = scores.std()\n",
    "        print(f\"  {metric.capitalize()}: {mean_score:.3f} (+/- {std_score * 2:.3f})\")\nelse:\n",
    "    print(\"  Neural Network: Cross-validation completed during training\")\n",
    "\n",
    "# Save production model\n",
    "if production_model_name != 'Neural Network' or not TENSORFLOW_AVAILABLE:\n",
    "    joblib.dump(production_model, MODELS / 'production_model.pkl')\n",
    "    print(f\"\\n💾 Production model saved as: production_model.pkl\")\nelse:\n",
    "    print(f\"\\n💾 Production model already saved as: neural_network_model.h5\")\n",
    "\n",
    "# Create model metadata\n",
    "model_metadata = {\n",
    "    'model_name': production_model_name,\n",
    "    'model_type': type(production_model).__name__,\n",
    "    'performance_metrics': production_metrics,\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features': list(X_train.columns),\n",
    "    'feature_count': len(X_train.columns),\n",
    "    'target_classes': [0, 1],\n",
    "    'model_version': '1.0',\n",
    "    'created_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "import json\n",
    "with open(MODELS / 'production_model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n📋 Model metadata saved\")\n",
    "print(\"\\n🎉 Week 6 Advanced Models Complete!\")\n",
    "print(\"\\n📋 Final Summary:\")\n",
    "print(f\"  🏆 Best Model: {production_model_name}\")\n",
    "print(f\"  📊 F1-Score: {production_metrics.get('f1_score', 'N/A'):.3f}\")\n",
    "print(f\"  🎯 Accuracy: {production_metrics.get('accuracy', 'N/A'):.3f}\")\n",
    "print(f\"  💾 Ready for production deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}