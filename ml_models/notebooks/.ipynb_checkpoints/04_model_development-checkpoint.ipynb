{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Model Development & Training\n",
    "\n",
    "This notebook covers:\n",
    "1. Baseline model development\n",
    "2. Decision Tree implementation\n",
    "3. Random Forest training\n",
    "4. XGBoost implementation\n",
    "5. Model evaluation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set up paths\n",
    "DATA_PROCESSED = Path('../data/processed')\n",
    "MODELS = Path('../models')\n",
    "REPORTS = Path('../reports/figures')\n",
    "\n",
    "print(\"‚úÖ Libraries imported and paths set up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed datasets\n",
    "print(\"üìä Loading preprocessed datasets...\")\n",
    "\n",
    "# Load scaled dataset (best for most ML algorithms)\n",
    "X_train = pd.read_csv(DATA_PROCESSED / 'scaled' / 'X_train.csv', index_col=0)\n",
    "X_test = pd.read_csv(DATA_PROCESSED / 'scaled' / 'X_test.csv', index_col=0)\n",
    "y_train = pd.read_csv(DATA_PROCESSED / 'scaled' / 'y_train.csv', index_col=0).squeeze()\n",
    "y_test = pd.read_csv(DATA_PROCESSED / 'scaled' / 'y_test.csv', index_col=0).squeeze()\n",
    "\n",
    "print(f\"‚úÖ Training set: {X_train.shape}\")\n",
    "print(f\"‚úÖ Test set: {X_test.shape}\")\n",
    "print(f\"‚úÖ Features: {list(X_train.columns)}\")\n",
    "print(f\"‚úÖ Target distribution - Train: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"‚úÖ Target distribution - Test: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüîç Training data preview:\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Baseline Logistic Regression\n",
    "print(\"üéØ MODEL 1: BASELINE LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "baseline_pred = baseline_model.predict(X_test)\n",
    "baseline_prob = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "baseline_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, baseline_pred),\n",
    "    'precision': precision_score(y_test, baseline_pred),\n",
    "    'recall': recall_score(y_test, baseline_pred),\n",
    "    'f1_score': f1_score(y_test, baseline_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, baseline_prob)\n",
    "}\n",
    "\n",
    "print(\"üìä Baseline Model Results:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, baseline_pred))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(baseline_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"\\nüîÑ Cross-validation accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(baseline_model, MODELS / 'baseline_logistic_regression.pkl')\n",
    "print(\"‚úÖ Baseline model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Decision Tree\n",
    "print(\"üå≥ MODEL 2: DECISION TREE CLASSIFIER\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "dt_params = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "print(\"üîç Performing hyperparameter tuning...\")\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    dt_params,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "dt_grid.fit(X_train, y_train)\n",
    "best_dt = dt_grid.best_estimator_\n",
    "\n",
    "print(f\"‚úÖ Best parameters: {dt_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best CV score: {dt_grid.best_score_:.3f}\")\n",
    "\n",
    "# Predictions\n",
    "dt_pred = best_dt.predict(X_test)\n",
    "dt_prob = best_dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "dt_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, dt_pred),\n",
    "    'precision': precision_score(y_test, dt_pred),\n",
    "    'recall': recall_score(y_test, dt_pred),\n",
    "    'f1_score': f1_score(y_test, dt_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, dt_prob)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Decision Tree Results:\")\n",
    "for metric, value in dt_metrics.items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_dt = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_dt.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüéØ Feature Importance (Decision Tree):\")\n",
    "print(feature_importance_dt)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_dt, MODELS / 'decision_tree_classifier.pkl')\n",
    "print(\"\\n‚úÖ Decision Tree model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest\n",
    "print(\"üå≤ MODEL 3: RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"üîç Performing hyperparameter tuning...\")\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_params,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "print(f\"‚úÖ Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best CV score: {rf_grid.best_score_:.3f}\")\n",
    "\n",
    "# Predictions\n",
    "rf_pred = best_rf.predict(X_test)\n",
    "rf_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "rf_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, rf_pred),\n",
    "    'precision': precision_score(y_test, rf_pred),\n",
    "    'recall': recall_score(y_test, rf_pred),\n",
    "    'f1_score': f1_score(y_test, rf_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, rf_prob)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Random Forest Results:\")\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüéØ Feature Importance (Random Forest):\")\n",
    "print(feature_importance_rf)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_rf, MODELS / 'random_forest_classifier.pkl')\n",
    "print(\"\\n‚úÖ Random Forest model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: XGBoost\n",
    "print(\"üöÄ MODEL 4: XGBOOST CLASSIFIER\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "xgb_params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "print(\"üîç Performing hyperparameter tuning...\")\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    xgb_params,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "\n",
    "print(f\"‚úÖ Best parameters: {xgb_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best CV score: {xgb_grid.best_score_:.3f}\")\n",
    "\n",
    "# Predictions\n",
    "xgb_pred = best_xgb.predict(X_test)\n",
    "xgb_prob = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "xgb_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, xgb_pred),\n",
    "    'precision': precision_score(y_test, xgb_pred),\n",
    "    'recall': recall_score(y_test, xgb_pred),\n",
    "    'f1_score': f1_score(y_test, xgb_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, xgb_prob)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä XGBoost Results:\")\n",
    "for metric, value in xgb_metrics.items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüéØ Feature Importance (XGBoost):\")\n",
    "print(feature_importance_xgb)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_xgb, MODELS / 'xgboost_classifier.pkl')\n",
    "print(\"\\n‚úÖ XGBoost model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "print(\"üìä MODEL COMPARISON\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Compile all results\n",
    "all_metrics = {\n",
    "    'Logistic Regression': baseline_metrics,\n",
    "    'Decision Tree': dt_metrics,\n",
    "    'Random Forest': rf_metrics,\n",
    "    'XGBoost': xgb_metrics\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_metrics).T\n",
    "comparison_df = comparison_df.round(3)\n",
    "\n",
    "print(\"\\nüìã Performance Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nüèÜ Best Models by Metric:\")\n",
    "for metric in comparison_df.columns:\n",
    "    best_model = comparison_df[metric].idxmax()\n",
    "    best_score = comparison_df[metric].max()\n",
    "    print(f\"  {metric.capitalize()}: {best_model} ({best_score:.3f})\")\n",
    "\n",
    "# Overall best model (by F1-score)\n",
    "best_overall = comparison_df['f1_score'].idxmax()\n",
    "print(f\"\\nüéØ Overall Best Model: {best_overall} (F1-Score: {comparison_df.loc[best_overall, 'f1_score']:.3f})\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv(REPORTS.parent / 'model_comparison_results.csv')\n",
    "print(\"\\nüíæ Comparison results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "print(\"üìä Creating visualizations...\")\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "ax1 = axes[0, 0]\n",
    "comparison_df[['accuracy', 'precision', 'recall', 'f1_score']].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Model Performance Comparison', fontweight='bold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. ROC Curves\n",
    "ax2 = axes[0, 1]\n",
    "models_data = [\n",
    "    ('Logistic Regression', baseline_prob),\n",
    "    ('Decision Tree', dt_prob),\n",
    "    ('Random Forest', rf_prob),\n",
    "    ('XGBoost', xgb_prob)\n",
    "]\n",
    "\n",
    "for name, prob in models_data:\n",
    "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
    "    auc_score = roc_auc_score(y_test, prob)\n",
    "    ax2.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curves Comparison', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature Importance (Random Forest)\n",
    "ax3 = axes[0, 2]\n",
    "top_features_rf = feature_importance_rf.head(8)\n",
    "ax3.barh(range(len(top_features_rf)), top_features_rf['importance'])\n",
    "ax3.set_yticks(range(len(top_features_rf)))\n",
    "ax3.set_yticklabels(top_features_rf['feature'])\n",
    "ax3.set_xlabel('Importance')\n",
    "ax3.set_title('Feature Importance (Random Forest)', fontweight='bold')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "# 4. Confusion Matrices\n",
    "models_pred = [\n",
    "    ('Logistic Regression', baseline_pred),\n",
    "    ('Decision Tree', dt_pred),\n",
    "    ('Random Forest', rf_pred),\n",
    "    ('XGBoost', xgb_pred)\n",
    "]\n",
    "\n",
    "for i, (name, pred) in enumerate(models_pred[:2]):\n",
    "    ax = axes[1, i]\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "    ax.set_title(f'{name} Confusion Matrix', fontweight='bold')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "# 5. F1-Score Comparison\n",
    "ax5 = axes[1, 2]\n",
    "f1_scores = comparison_df['f1_score'].sort_values(ascending=True)\n",
    "colors = ['red' if x == f1_scores.max() else 'skyblue' for x in f1_scores]\n",
    "ax5.barh(range(len(f1_scores)), f1_scores.values, color=colors)\n",
    "ax5.set_yticks(range(len(f1_scores)))\n",
    "ax5.set_yticklabels(f1_scores.index)\n",
    "ax5.set_xlabel('F1-Score')\n",
    "ax5.set_title('F1-Score Comparison', fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS / 'model_comparison_visualizations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary and recommendations\n",
    "print(\"üìã MODEL DEVELOPMENT SUMMARY\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create summary report\n",
    "summary_report = {\n",
    "    'Dataset Info': {\n",
    "        'Training Samples': len(X_train),\n",
    "        'Test Samples': len(X_test),\n",
    "        'Features': len(X_train.columns),\n",
    "        'Target Classes': len(y_train.unique())\n",
    "    },\n",
    "    'Models Trained': {\n",
    "        'Baseline': 'Logistic Regression',\n",
    "        'Tree-based': 'Decision Tree, Random Forest',\n",
    "        'Boosting': 'XGBoost',\n",
    "        'Total Models': 4\n",
    "    },\n",
    "    'Best Performance': {\n",
    "        'Best Accuracy': f\"{comparison_df['accuracy'].max():.3f} ({comparison_df['accuracy'].idxmax()})\",\n",
    "        'Best Precision': f\"{comparison_df['precision'].max():.3f} ({comparison_df['precision'].idxmax()})\",\n",
    "        'Best Recall': f\"{comparison_df['recall'].max():.3f} ({comparison_df['recall'].idxmax()})\",\n",
    "        'Best F1-Score': f\"{comparison_df['f1_score'].max():.3f} ({comparison_df['f1_score'].idxmax()})\",\n",
    "        'Best ROC-AUC': f\"{comparison_df['roc_auc'].max():.3f} ({comparison_df['roc_auc'].idxmax()})\"\n",
    "    },\n",
    "    'Recommendations': {\n",
    "        'Production Model': best_overall,\n",
    "        'Reason': 'Highest F1-Score (balanced precision and recall)',\n",
    "        'Alternative': comparison_df['accuracy'].idxmax() if comparison_df['accuracy'].idxmax() != best_overall else 'Random Forest'\n",
    "    }\n",
    "}\n",
    "\n",
    "for section, info in summary_report.items():\n",
    "    print(f\"\\nüìä {section}:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save summary report\n",
    "import json\n",
    "with open(REPORTS.parent / 'model_development_summary.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nüíæ Model development summary saved\")\n",
    "print(\"\\nüéâ Week 5 Model Development Complete!\")\n",
    "print(\"\\nüìã Next Steps (Week 6):\")\n",
    "print(\"  1. Ensemble model creation\")\n",
    "print(\"  2. Neural network implementation\")\n",
    "print(\"  3. Model optimization and fine-tuning\")\n",
    "print(\"  4. Final model selection and validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}